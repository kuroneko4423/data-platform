# ğŸ”„ ETL Node

ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ãƒãƒ¼ãƒ‰ã§ã™ã€‚

## ğŸ—ï¸ æ§‹æˆã‚µãƒ¼ãƒ“ã‚¹

| ã‚µãƒ¼ãƒ“ã‚¹ | ãƒãƒ¼ãƒˆ | ç”¨é€” |
|---------|--------|------|
| Airflow Webserver | 8080 | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†UI |
| Airflow Scheduler | - | DAGã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° |
| Airflow Worker | - | ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ |
| Flower | 5555 | Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ç›£è¦–UI |

## ğŸ“‹ å‰ææ¡ä»¶

**ä»¥ä¸‹ã®ãƒãƒ¼ãƒ‰ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨:**
- Storage Nodeï¼ˆPostgreSQL, Redisï¼‰
- DWH Nodeï¼ˆClickHouseï¼‰â€»dbtå®Ÿè¡Œæ™‚ã«å¿…è¦

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸
chmod +x setup.sh

# 2. Dockerã®ç¢ºèª
./setup.sh check

# 3. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ»æ¨©é™è¨­å®š
./setup.sh init

# 4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆStorage/DWH Node IPã‚’å…¥åŠ›ï¼‰
./setup.sh configure

# 5. ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•
./setup.sh start
```

## ğŸ“‹ ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§

```bash
./setup.sh check      # Dockerã®ç¢ºèª
./setup.sh init       # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ»æ¨©é™è¨­å®š
./setup.sh configure  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
./setup.sh start      # ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•
./setup.sh stop       # ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢
./setup.sh restart    # ã‚µãƒ¼ãƒ“ã‚¹ã‚’å†èµ·å‹•
./setup.sh status     # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
./setup.sh logs       # å…¨ãƒ­ã‚°è¡¨ç¤º
./setup.sh logs airflow-webserver  # Webserverãƒ­ã‚°
./setup.sh reset      # å…¨ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
```

## ğŸ” ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±

### Airflow
- **URL**: http://<ã“ã®VMã®IP>:8080
- **User**: admin
- **Password**: admin123

### Flower
- **URL**: http://<ã“ã®VMã®IP>:5555
- èªè¨¼ãªã—

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
etl-node/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ setup.sh
â”œâ”€â”€ README.md
â”œâ”€â”€ dags/                    # Airflowã®DAGãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ sample_etl_pipeline.py
â”œâ”€â”€ logs/                    # Airflowãƒ­ã‚°
â”œâ”€â”€ plugins/                 # Airflowãƒ—ãƒ©ã‚°ã‚¤ãƒ³
â”œâ”€â”€ config/                  # Airflowè¨­å®š
â””â”€â”€ dbt/                     # dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
    â”œâ”€â”€ profiles.yml.template
    â”œâ”€â”€ dbt_project.yml
    â””â”€â”€ models/
        â”œâ”€â”€ bronze/
        â”œâ”€â”€ silver/
        â”œâ”€â”€ gold/
        â””â”€â”€ marts/
```

## ğŸ“Š dbtä½¿ç”¨æ–¹æ³•

```bash
# Airflowã‚³ãƒ³ãƒ†ãƒŠå†…ã§dbtã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
docker exec -it airflow-worker bash

# dbtå®Ÿè¡Œ
cd /opt/airflow/dbt
dbt run --profiles-dir .
dbt test --profiles-dir .
```

## ğŸ”— Airflowæ¥ç¶šè¨­å®š

èµ·å‹•æ™‚ã«ä»¥ä¸‹ã®æ¥ç¶šãŒè‡ªå‹•è¨­å®šã•ã‚Œã¾ã™ï¼š

| Connection ID | æ¥ç¶šå…ˆ |
|--------------|--------|
| minio | Storage Node MinIO |
| clickhouse | DWH Node ClickHouse |
| postgres | Storage Node PostgreSQL |

## âš™ï¸ .envè¨­å®š

```bash
# Airflowè¨­å®š
AIRFLOW_UID=50000
AIRFLOW_USER=admin
AIRFLOW_PASSWORD=admin123

# è¿½åŠ Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
_PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-amazon ...

# æ¥ç¶šè¨­å®šï¼ˆå¿…é ˆï¼‰
STORAGE_NODE_IP=10.10.10.10
DWH_NODE_IP=10.10.10.20
```

## âš ï¸ æ³¨æ„äº‹é …

1. Storage NodeãŒå…ˆã«èµ·å‹•ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆPostgreSQL, Redisä½¿ç”¨ï¼‰
2. åˆå›èµ·å‹•æ™‚ã¯DBåˆæœŸåŒ–ã®ãŸã‚æ•°åˆ†ã‹ã‹ã‚Šã¾ã™
3. DAGãƒ•ã‚¡ã‚¤ãƒ«ã¯`dags/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„
4. dbtå®Ÿè¡Œã«ã¯DWH NodeãŒå¿…è¦ã§ã™
